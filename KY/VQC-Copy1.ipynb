{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane.optimize import GradientDescentOptimizer\n",
    "from pennylane import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Integration... Let's start with just 1D integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('toy_data_10e4.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.55258737 0.41757623 2.5810235  0.        ]\n",
      " [1.05945455 2.73902844 2.59982768 0.01002004]\n",
      " [2.71202945 3.38179615 1.0272925  0.02004008]\n",
      " ...\n",
      " [2.26058831 2.62355914 0.18093676 4.97995992]\n",
      " [2.91426381 3.41772047 2.93088925 4.98997996]\n",
      " [0.1022243  2.0529486  1.72946409 5.        ]]\n"
     ]
    }
   ],
   "source": [
    "Xdata = data['xdata']\n",
    "print(Xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03413077  0.00653628 -0.83397146 ...  0.9999308  -0.8789377\n",
      " -0.79724696]\n"
     ]
    }
   ],
   "source": [
    "Ydata = data['ydata']\n",
    "print(Ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kban/.local/lib/python3.8/site-packages/pennylane_lightning/lightning_qubit/lightning_qubit.py:822: UserWarning: Pre-compiled binaries for lightning.qubit are not available. Falling back to using the Python-based default.qubit implementation. To manually compile from source, follow the instructions at https://pennylane-lightning.readthedocs.io/en/latest/installation.html.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "dev_stochastic = qml.device(\"lightning.qubit\", wires=10)\n",
    "\n",
    "@qml.qnode(dev_stochastic, diff_method=\"parameter-shift\")\n",
    "def single_data_point_ansatz(params, phis, x, num_layers):\n",
    "    \"\"\"\n",
    "    Quantum Circuit Model for a single data point x.\n",
    "\n",
    "    INPUT\n",
    "    params : array of theta parameters\n",
    "    phis : array of phi for the last rotation gate\n",
    "    x : single data point for reuploading\n",
    "\n",
    "    OUTPUT\n",
    "    Expectation value with PauliZ measure\n",
    "    \"\"\"\n",
    "    \n",
    "    num_qubits = math.ceil(len(x) / 2)\n",
    "    param_index = 0\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        for i in range(len(x)):\n",
    "            qubit = i // 2\n",
    "            qml.RY(params[param_index], wires=qubit)\n",
    "            qml.RZ(params[param_index + 1], wires=qubit)\n",
    "            qml.RZ(x[i], wires=qubit)  # Re-uploading data here\n",
    "            qml.RY(params[param_index + 2], wires=qubit)\n",
    "            qml.RZ(params[param_index + 3], wires=qubit)\n",
    "            param_index += 4\n",
    "        \n",
    "        if num_qubits > 1:\n",
    "            for q in range(0, num_qubits - 1, 1):\n",
    "                qml.CZ(wires=[q, q + 1])\n",
    "            if num_qubits > 2:\n",
    "                qml.CZ(wires=[num_qubits - 1, 0])\n",
    "    \n",
    "    for i in range(num_qubits): \n",
    "        qml.RY(phis[i], wires=i)\n",
    "\n",
    "    obs = qml.PauliZ(0)\n",
    "    for i in range(num_qubits-1):\n",
    "         obs = obs @ qml.PauliZ(i+1)\n",
    "    \n",
    "    return qml.expval(obs)\n",
    "\n",
    "\n",
    "def batch_ansatz(params, phis, xdata, num_layers):\n",
    "    \"\"\"\n",
    "    Quantum Circuit Model for batch data.\n",
    "\n",
    "    INPUT\n",
    "    params : array of theta parameters\n",
    "    phis : array of phi for the last rotation gate\n",
    "    xdata : batch of data points for reuploading\n",
    "    num_layers : layers we will append for the circuit\n",
    "\n",
    "    OUTPUT\n",
    "    Mean of expectation values with PauliZ measure over the batch\n",
    "    \"\"\"\n",
    "    # Ensure that `params` is a flat list for single_data_point_ansatz\n",
    "    flat_params = params.flatten()\n",
    "    \n",
    "    # Calculate the expectation for each data point and take the mean\n",
    "    expectations = [single_data_point_ansatz(flat_params, phis, x, num_layers) for x in xdata]\n",
    "    return np.mean(expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17827215 0.57666515 0.67513005 0.81846239 0.93932721 0.26357393\n",
      " 0.20275885 0.8592312  0.26100854 0.95916566 0.73823606 0.98867133\n",
      " 0.76168891 0.55937236 0.12678064 0.04602907 0.97936243 0.6393217\n",
      " 0.28326717 0.56462284 0.10534565 0.62418791 0.02774595 0.26683968\n",
      " 0.78912964 0.00125882 0.32445339 0.46856635 0.12562493 0.47928415\n",
      " 0.82211142 0.70636646 0.26782975 0.17742842 0.47312994 0.9278913\n",
      " 0.11108655 0.2551302  0.12350515 0.27610254]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xdim = 4\n",
    "num_qubits = math.ceil(xdim / 2)\n",
    "num_layers = 2\n",
    "\n",
    "xdata = Xdata[:100] #np.random.uniform(size = [100,xdim], requires_grad=False)\n",
    "thetas = np.random.uniform(size = xdim*num_layers*5,requires_grad = True)\n",
    "phis = np.random.uniform(size=(num_qubits,1), requires_grad =True)\n",
    "target = Ydata[:100]\n",
    "print(thetas)\n",
    "\n",
    "params = 0\n",
    "params += thetas\n",
    "param_index = 1\n",
    "for _ in range(num_layers):\n",
    "    for j in range(xdim):\n",
    "        params[param_index] = thetas[param_index] * xdata[0][j]\n",
    "        param_index += 5\n",
    "\n",
    "fig, ax = qml.draw_mpl(single_data_point_ansatz)(params, phis, xdata[0], num_layers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 정의\n",
    "def cost_function(params, phis, xdata, target, num_layers):\n",
    "    # 손실 계산을 위해 batch_ansatz를 사용합니다.\n",
    "    predictions = batch_ansatz(params, phis, xdata, num_layers)\n",
    "    loss = np.mean((predictions - target)**2)\n",
    "    return loss\n",
    "\n",
    "# 수동 그라디언트 계산 함수\n",
    "def parameter_shift_term(qnode, params, i, phis, xdata, num_layers):\n",
    "    shifted_params = params.copy()\n",
    "    shift = np.pi / 2\n",
    "    \n",
    "    # 각 파라미터에 대한 기울기를 계산합니다.\n",
    "    shifted_params = params.copy()\n",
    "    shifted_params[i] += shift\n",
    "    forward = [qnode(shifted_params, phis, x, num_layers) for x in xdata]  # Forward evaluation\n",
    "\n",
    "    shifted_params[i] -= 2 * shift\n",
    "    backward = [qnode(shifted_params, phis, x, num_layers) for x in xdata]  # Backward evaluation\n",
    "\n",
    "    # 배치 기대값의 평균을 구하여 기울기를 계산합니다.\n",
    "    gradient = np.mean([(f - b) / (2 * np.sin(shift)) for f, b in zip(forward, backward)])\n",
    "    return gradient\n",
    "\n",
    "# 파라미터 업데이트 함수\n",
    "def update_params(params, grads, learning_rate=0.01):\n",
    "    new_params = params - learning_rate * grads\n",
    "    return new_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.52488411646778\n",
      "Epoch 2, Loss: 0.5246270315447856\n",
      "Epoch 3, Loss: 0.5243728564555415\n",
      "Epoch 4, Loss: 0.5241215977913665\n",
      "Epoch 5, Loss: 0.5238732620573885\n",
      "Epoch 6, Loss: 0.5236278556720805\n",
      "Epoch 7, Loss: 0.5233853849668008\n",
      "Epoch 8, Loss: 0.5231458561853439\n",
      "Epoch 9, Loss: 0.5229092754834932\n",
      "Epoch 10, Loss: 0.5226756489285845\n",
      "Epoch 11, Loss: 0.5224449824990738\n",
      "Epoch 12, Loss: 0.5222172820841131\n",
      "Epoch 13, Loss: 0.5219925534831327\n",
      "Epoch 14, Loss: 0.5217708024054305\n",
      "Epoch 15, Loss: 0.5215520344697694\n",
      "Epoch 16, Loss: 0.521336255203979\n",
      "Epoch 17, Loss: 0.521123470044568\n",
      "Epoch 18, Loss: 0.5209136843363408\n",
      "Epoch 19, Loss: 0.5207069033320231\n",
      "Epoch 20, Loss: 0.5205031321918939\n",
      "Epoch 21, Loss: 0.5203023759834255\n",
      "Epoch 22, Loss: 0.5201046396809305\n",
      "Epoch 23, Loss: 0.5199099281652153\n",
      "Epoch 24, Loss: 0.5197182462232437\n",
      "Epoch 25, Loss: 0.5195295985478046\n",
      "Epoch 26, Loss: 0.5193439897371901\n",
      "Epoch 27, Loss: 0.5191614242948795\n",
      "Epoch 28, Loss: 0.5189819066292324\n",
      "Epoch 29, Loss: 0.5188054410531869\n",
      "Epoch 30, Loss: 0.5186320317839688\n",
      "Epoch 31, Loss: 0.5184616829428057\n",
      "Epoch 32, Loss: 0.5182943985546514\n",
      "Epoch 33, Loss: 0.5181301825479161\n",
      "Epoch 34, Loss: 0.5179690387542049\n",
      "Epoch 35, Loss: 0.5178109709080653\n",
      "Epoch 36, Loss: 0.5176559826467411\n",
      "Epoch 37, Loss: 0.5175040775099357\n",
      "Epoch 38, Loss: 0.517355258939582\n",
      "Epoch 39, Loss: 0.5172095302796212\n",
      "Epoch 40, Loss: 0.51706689477579\n",
      "Epoch 41, Loss: 0.5169273555754139\n",
      "Epoch 42, Loss: 0.5167909157272114\n",
      "Epoch 43, Loss: 0.516657578181104\n",
      "Epoch 44, Loss: 0.5165273457880355\n",
      "Epoch 45, Loss: 0.516400221299799\n",
      "Epoch 46, Loss: 0.5162762073688726\n",
      "Epoch 47, Loss: 0.5161553065482622\n",
      "Epoch 48, Loss: 0.5160375212913542\n",
      "Epoch 49, Loss: 0.5159228539517744\n",
      "Epoch 50, Loss: 0.515811306783257\n",
      "Epoch 51, Loss: 0.5157028819395209\n",
      "Epoch 52, Loss: 0.5155975814741537\n",
      "Epoch 53, Loss: 0.5154954073405057\n",
      "Epoch 54, Loss: 0.5153963613915906\n",
      "Epoch 55, Loss: 0.5153004453799952\n",
      "Epoch 56, Loss: 0.5152076609577969\n",
      "Epoch 57, Loss: 0.5151180096764905\n",
      "Epoch 58, Loss: 0.5150314929869226\n",
      "Epoch 59, Loss: 0.5149481122392336\n",
      "Epoch 60, Loss: 0.51486786868281\n",
      "Epoch 61, Loss: 0.5147907634662423\n",
      "Epoch 62, Loss: 0.5147167976372947\n",
      "Epoch 63, Loss: 0.5146459721428799\n",
      "Epoch 64, Loss: 0.5145782878290429\n",
      "Epoch 65, Loss: 0.5145137454409556\n",
      "Epoch 66, Loss: 0.514452345622916\n",
      "Epoch 67, Loss: 0.5143940889183587\n",
      "Epoch 68, Loss: 0.5143389757698715\n",
      "Epoch 69, Loss: 0.5142870065192224\n",
      "Epoch 70, Loss: 0.5142381814073924\n",
      "Epoch 71, Loss: 0.514192500574619\n",
      "Epoch 72, Loss: 0.5141499640604468\n",
      "Epoch 73, Loss: 0.5141105718037849\n",
      "Epoch 74, Loss: 0.5140743236429768\n",
      "Epoch 75, Loss: 0.5140412193158729\n",
      "Epoch 76, Loss: 0.5140112584599152\n",
      "Epoch 77, Loss: 0.5139844406122294\n",
      "Epoch 78, Loss: 0.5139607652097239\n",
      "Epoch 79, Loss: 0.5139402315891983\n",
      "Epoch 80, Loss: 0.5139228389874588\n",
      "Epoch 81, Loss: 0.5139085865414432\n",
      "Epoch 82, Loss: 0.5138974732883519\n",
      "Epoch 83, Loss: 0.5138894981657895\n",
      "Epoch 84, Loss: 0.513884660011912\n",
      "Epoch 85, Loss: 0.5138829575655832\n",
      "Epoch 86, Loss: 0.5138843894665386\n",
      "Epoch 87, Loss: 0.5138889542555583\n",
      "Epoch 88, Loss: 0.5138966503746459\n",
      "Epoch 89, Loss: 0.5139074761672172\n",
      "Epoch 90, Loss: 0.5139214298782946\n",
      "Epoch 91, Loss: 0.5139385096547122\n",
      "Epoch 92, Loss: 0.5139587135453246\n",
      "Epoch 93, Loss: 0.5139820395012286\n",
      "Epoch 94, Loss: 0.5140084853759869\n",
      "Epoch 95, Loss: 0.5140380489258639\n",
      "Epoch 96, Loss: 0.5140707278100675\n",
      "Epoch 97, Loss: 0.514106519590997\n",
      "Epoch 98, Loss: 0.5141454217345016\n",
      "Epoch 99, Loss: 0.5141874316101438\n",
      "Epoch 100, Loss: 0.5142325464914707\n",
      "최적화된 파라미터: [ 0.15607132  1.47402017  0.72290955  0.79989274  0.99058456  0.26710117\n",
      "  0.06851127  0.86514754  0.29650405  0.95894027  0.71773453  2.55925181\n",
      "  0.77393856  0.55180841  0.14714773  0.03901054 -0.04325455  0.64508094\n",
      "  0.30143918  0.56307769  0.13412098  1.59038136  0.02755718  0.26424824\n",
      "  0.82906055  0.04303178  0.06846432  0.51168225  0.0610638   0.52015859\n",
      "  0.80665774  1.8013391   0.26782975  0.17742842  0.47312994  0.9278913\n",
      "  0.          0.2551302   0.12350515  0.27610254]\n"
     ]
    }
   ],
   "source": [
    "# 학습 설정\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    # 그라디언트를 저장할 배열 초기화\n",
    "    grads = np.zeros_like(params.flatten())  # params.flatten()으로 모든 파라미터에 대한 평평한 배열 생성\n",
    "\n",
    "    # 모든 파라미터에 대해 그라디언트 계산\n",
    "    for i in range(len(params.flatten())):\n",
    "        # 각 파라미터에 대한 기울기 계산\n",
    "        grads[i] = parameter_shift_term(single_data_point_ansatz, params, i, phis, xdata, num_layers)\n",
    "    \n",
    "    # 그라디언트의 모양을 원래 params의 모양으로 재구성\n",
    "    grads = grads.reshape(params.shape)\n",
    "    \n",
    "    # 파라미터 업데이트\n",
    "    params = update_params(params, grads, learning_rate)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = cost_function(params, phis, xdata, target, num_layers)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "\n",
    "# 최종 파라미터 출력\n",
    "print(\"최적화된 파라미터:\", params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cost_function() missing 2 required positional arguments: 'target' and 'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Using Scipy's minimize function with the LBFGS method\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Extract the optimized parameters\u001b[39;00m\n\u001b[1;32m     28\u001b[0m params \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py:305\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 305\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    311\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    328\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "\u001b[0;31mTypeError\u001b[0m: cost_function() missing 2 required positional arguments: 'target' and 'num_layers'"
     ]
    }
   ],
   "source": [
    "# 학습 설정\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    # 그라디언트를 저장할 배열 초기화\n",
    "    grads = np.zeros_like(params.flatten())  # params.flatten()으로 모든 파라미터에 대한 평평한 배열 생성\n",
    "\n",
    "    # 모든 파라미터에 대해 그라디언트 계산\n",
    "    for i in range(len(params.flatten())):\n",
    "        # 각 파라미터에 대한 기울기 계산\n",
    "        grads[i] = parameter_shift_term(single_data_point_ansatz, params, i, phis, xdata, num_layers)\n",
    "    \n",
    "    # 그라디언트의 모양을 원래 params의 모양으로 재구성\n",
    "    grads = grads.reshape(params.shape)\n",
    "    \n",
    "    # 파라미터 업데이트    \n",
    "    params = params.flatten()\n",
    "    target = target.flatten()\n",
    "\n",
    "    # Using Scipy's minimize function with the LBFGS method\n",
    "    result = minimize(cost_function, params, args=(xdata, target), method='L-BFGS-B')\n",
    "\n",
    "    # Extract the optimized parameters\n",
    "    params = result.x\n",
    "    \n",
    "\n",
    "    # 손실 계산\n",
    "    loss = cost_function(params, phis, xdata, target, num_layers)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "\n",
    "# 최종 파라미터 출력\n",
    "print(\"최적화된 파라미터:\", params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = 4\n",
    "num_qubits = math.ceil(xdim / 2)\n",
    "num_layers = 1\n",
    "\n",
    "xdata = np.random.uniform(size=(1, xdim), requires_grad=False)\n",
    "thetas = np.random.uniform(size=xdim*num_layers*5, requires_grad=True)\n",
    "phis = np.random.uniform(size=(num_qubits,1), requires_grad =True)\n",
    "\n",
    "params = 0\n",
    "params += thetas\n",
    "\n",
    "\n",
    "for i in range(len(Xdata)):\n",
    "    param_index = 1\n",
    "    for _ in range(num_layers):\n",
    "        for j in range(xdim):\n",
    "            params[param_index] = thetas[param_index] * Xdata[i][j]\n",
    "            param_index += 5\n",
    "\n",
    "    # print(Ansatz(params, phis, xdim, num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13852134]\n"
     ]
    }
   ],
   "source": [
    "def parameter_shift_term(qnode, params, x_i, phis, xdim, num_layers):\n",
    "    \"\"\"\n",
    "    Basic Parameter shift rule for each x_i i= 0,1, ..., xdim\n",
    "   \n",
    "    INPUT\n",
    "    qnode :  circuit ansatz we designed\n",
    "    parmas : (array) of parameters we put in the circuit\n",
    "    x_i : (int) ${{\\partial f(\\theta)} \\over {\\partial x_i}}$\n",
    "    phis : (array) the last parameters of the circuit\n",
    "    ndim : (int) dimenstion of $\\vec{x}$\n",
    "    num_layers : (int) number of layer we make for the model\n",
    "\n",
    "    OUTPUT\n",
    "    expectation value where we apply the Basic PSR for the circuit\n",
    "    \"\"\"\n",
    "\n",
    "    shifted = params.copy()\n",
    "    i = x_i*5\n",
    "    for _ in range(num_layers):\n",
    "\n",
    "        shifted[i: i+5] += np.pi/2\n",
    "        forward = qnode(shifted, phis, xdim, num_layers)  # forward evaluation\n",
    "\n",
    "        shifted[i: i+5] -= np.pi/2\n",
    "        backward = qnode(shifted, phis, xdim, num_layers) # backward evaluation\n",
    "        \n",
    "        i += xdim*5\n",
    "\n",
    "    return 0.5 * (forward - backward)\n",
    "\n",
    "# gradient with respect to  x_0\n",
    "print(parameter_shift_term(Ansatz, params, 0, phis, xdim, num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13852134 -0.10568801  0.0054801   0.11752005]\n"
     ]
    }
   ],
   "source": [
    "def parameter_shift(qnode, params, phis, xdim, num_layers):\n",
    "    \"\"\"\n",
    "    PSR applying to the whole circuit and getting the gradient vector as array\n",
    "\n",
    "    INPUT\n",
    "    qnode : circuit model\n",
    "    params : parameters that are in the circuit except the last layer\n",
    "    phis : parameters for the last RY gate\n",
    "    xdim : dimension of $\\vec{x}$\n",
    "    num_layers : number of layers\n",
    "\n",
    "    OUTPUT\n",
    "    gradient vector of the quantum circuit\n",
    "    \"\"\"\n",
    "    gradients = np.zeros([xdim])\n",
    "\n",
    "    for i in range(xdim):\n",
    "        gradients[i] = parameter_shift_term(qnode, params, i, phis, xdim, num_layers)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "print(parameter_shift(Ansatz, params, phis, xdim, num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (4069589838.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def loss_function(qnode = Ansatz, thetas, phis, xdata=Xdata[0], xdim, num_layers, target):\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def loss_function(qnode = Ansatz, thetas, phis, xdata=Xdata[0], xdim, num_layers, target):\n",
    "    \"\"\"\n",
    "    loss function\n",
    "\n",
    "    INPUT\n",
    "    \n",
    "\n",
    "    OUTPUT\n",
    "    loss with MSE\n",
    "    \"\"\"\n",
    "    \n",
    "    params = 0\n",
    "    params += thetas\n",
    "    param_index = 1\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        for j in range(xdim):\n",
    "            params[param_index] = thetas[param_index] * xdata[j]\n",
    "            param_index += 5\n",
    "\n",
    "\n",
    "    gradient = parameter_shift(Ansatz, params, phis, xdim, num_layers)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
